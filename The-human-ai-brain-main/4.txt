
// FDQC v3.1 - CUDA-Accelerated Implementation
// Compile: nvcc -std=c++17 -O3 –use_fast_math -arch=sm_75 fdqc_validation_cuda.cu -o fdqc_validation_cuda

#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <curand.h>
#include <iostream>
#include <vector>
#include <memory>
#include <chrono>

// ============================================================================
// CUDA Error Checking
// ============================================================================

#define CUDA_CHECK(call)   
do {   
cudaError_t err = call;   
if (err != cudaSuccess) {   
std::cerr << “CUDA error at “ << **FILE** << “:” << **LINE** << “ - “   
<< cudaGetErrorString(err) << std::endl;   
exit(EXIT_FAILURE);   
}   
} while(0)

#define CUBLAS_CHECK(call)   
do {   
cublasStatus_t status = call;   
if (status != CUBLAS_STATUS_SUCCESS) {   
std::cerr << “cuBLAS error at “ << **FILE** << “:” << **LINE** << std::endl;   
exit(EXIT_FAILURE);   
}   
} while(0)

// ============================================================================
// CUDA Kernels
// ============================================================================

// GELU activation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
**global** void gelu_kernel(float* data, size_t n) {
size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
if (idx < n) {
float x = data[idx];
float x3 = x * x * x;
float inner = 0.7978845608f * (x + 0.044715f * x3);
data[idx] = 0.5f * x * (1.0f + tanhf(inner));
}
}

// ReLU activation
**global** void relu_kernel(float* data, size_t n) {
size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
if (idx < n) {
data[idx] = fmaxf(0.0f, data[idx]);
}
}

// Tanh activation (safe clamping)
**global** void tanh_kernel(float* data, size_t n) {
size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
if (idx < n) {
data[idx] = tanhf(fminf(fmaxf(data[idx], -10.0f), 10.0f));
}
}

// Layer Normalization
**global** void layer_norm_kernel(
float* output, const float* input,
const float* gamma, const float* beta,
size_t batch_size, size_t features, float eps
) {
size_t b = blockIdx.x;
if (b >= batch_size) return;

```
// Compute mean
float mean = 0.0f;
for (size_t f = 0; f < features; ++f) {
    mean += input[b * features + f];
}
mean /= features;

// Compute variance
float var = 0.0f;
for (size_t f = 0; f < features; ++f) {
    float diff = input[b * features + f] - mean;
    var += diff * diff;
}
var /= features;

// Normalize
float std = sqrtf(var + eps);
size_t f = threadIdx.x;
if (f < features) {
    float normalized = (input[b * features + f] - mean) / std;
    output[b * features + f] = gamma[f] * normalized + beta[f];
}
```

}

// Softmax (per-row)
**global** void softmax_kernel(float* data, size_t batch_size, size_t n_classes) {
size_t b = blockIdx.x;
if (b >= batch_size) return;

```
float* row = data + b * n_classes;

// Find max for numerical stability
float max_val = row[0];
for (size_t c = 1; c < n_classes; ++c) {
    max_val = fmaxf(max_val, row[c]);
}

// Exp and sum
float sum = 0.0f;
for (size_t c = 0; c < n_classes; ++c) {
    row[c] = expf(row[c] - max_val);
    sum += row[c];
}

// Normalize
for (size_t c = 0; c < n_classes; ++c) {
    row[c] /= sum;
}
```

}

// Entropy computation
**global** void entropy_kernel(
float* entropy_out, const float* probs,
size_t batch_size, size_t n_dim
) {
size_t b = blockIdx.x * blockDim.x + threadIdx.x;
if (b >= batch_size) return;

```
float ent = 0.0f;
for (size_t d = 0; d < n_dim; ++d) {
    float p = fmaxf(probs[b * n_dim + d], 1e-10f);
    ent -= p * log2f(p);
}
entropy_out[b] = ent;
```

}

// MSE Loss
**global** void mse_loss_kernel(
float* loss_out, const float* pred, const float* target, size_t n
) {
**shared** float shared_sum[256];

```
size_t tid = threadIdx.x;
size_t idx = blockIdx.x * blockDim.x + tid;

float sum = 0.0f;
if (idx < n) {
    float diff = pred[idx] - target[idx];
    sum = diff * diff;
}

shared_sum[tid] = sum;
__syncthreads();

// Reduction
for (size_t s = blockDim.x / 2; s > 0; s >>= 1) {
    if (tid < s) {
        shared_sum[tid] += shared_sum[tid + s];
    }
    __syncthreads();
}

if (tid == 0) {
    atomicAdd(loss_out, shared_sum[0]);
}
```

}

// Cross-Entropy Loss
**global** void cross_entropy_kernel(
float* loss_out, const float* probs, const int* labels,
size_t batch_size, size_t n_classes
) {
size_t b = blockIdx.x * blockDim.x + threadIdx.x;
if (b >= batch_size) return;

```
int label = labels[b];
float prob = fmaxf(probs[b * n_classes + label], 1e-10f);
atomicAdd(loss_out, -logf(prob));
```

}

// Accuracy computation
**global** void accuracy_kernel(
int* correct_out, const float* logits, const int* labels,
size_t batch_size, size_t n_classes
) {
size_t b = blockIdx.x * blockDim.x + threadIdx.x;
if (b >= batch_size) return;

```
// Find argmax
int pred_label = 0;
float max_prob = logits[b * n_classes];
for (size_t c = 1; c < n_classes; ++c) {
    if (logits[b * n_classes + c] > max_prob) {
        max_prob = logits[b * n_classes + c];
        pred_label = c;
    }
}

if (pred_label == labels[b]) {
    atomicAdd(correct_out, 1);
}
```

}

// ============================================================================
// CUDA Tensor Wrapper
// ============================================================================

class CUDATensor {
private:
float* d_data;
size_t size_;
std::vector<size_t> shape_;

public:
CUDATensor() : d_data(nullptr), size_(0) {}

```
CUDATensor(std::vector<size_t> shape) : shape_(shape) {
    size_ = 1;
    for (auto s : shape) size_ *= s;
    CUDA_CHECK(cudaMalloc(&d_data, size_ * sizeof(float)));
    CUDA_CHECK(cudaMemset(d_data, 0, size_ * sizeof(float)));
}

~CUDATensor() {
    if (d_data) cudaFree(d_data);
}

// Move semantics
CUDATensor(CUDATensor&& other) noexcept 
    : d_data(other.d_data), size_(other.size_), shape_(std::move(other.shape_)) {
    other.d_data = nullptr;
}

CUDATensor& operator=(CUDATensor&& other) noexcept {
    if (this != &other) {
        if (d_data) cudaFree(d_data);
        d_data = other.d_data;
        size_ = other.size_;
        shape_ = std::move(other.shape_);
        other.d_data = nullptr;
    }
    return *this;
}

float* data() { return d_data; }
const float* data() const { return d_data; }
size_t size() const { return size_; }
size_t dim(size_t i) const { return i < shape_.size() ? shape_[i] : 1; }

void copy_from_host(const std::vector<float>& host_data) {
    CUDA_CHECK(cudaMemcpy(d_data, host_data.data(), 
                          size_ * sizeof(float), cudaMemcpyHostToDevice));
}

void copy_to_host(std::vector<float>& host_data) const {
    host_data.resize(size_);
    CUDA_CHECK(cudaMemcpy(host_data.data(), d_data, 
                          size_ * sizeof(float), cudaMemcpyDeviceToHost));
}

void fill(float value) {
    std::vector<float> host_data(size_, value);
    copy_from_host(host_data);
}

void apply_gelu() {
    dim3 block(256);
    dim3 grid((size_ + block.x - 1) / block.x);
    gelu_kernel<<<grid, block>>>(d_data, size_);
    CUDA_CHECK(cudaGetLastError());
}

void apply_relu() {
    dim3 block(256);
    dim3 grid((size_ + block.x - 1) / block.x);
    relu_kernel<<<grid, block>>>(d_data, size_);
    CUDA_CHECK(cudaGetLastError());
}

void apply_tanh() {
    dim3 block(256);
    dim3 grid((size_ + block.x - 1) / block.x);
    tanh_kernel<<<grid, block>>>(d_data, size_);
    CUDA_CHECK(cudaGetLastError());
}

void apply_softmax(size_t batch_size) {
    dim3 block(1);
    dim3 grid(batch_size);
    softmax_kernel<<<grid, block>>>(d_data, batch_size, shape_[1]);
    CUDA_CHECK(cudaGetLastError());
}
```

};

// ============================================================================
// CUDA Linear Layer
// ============================================================================

class CUDALinear {
private:
CUDATensor weight;  // [out_features, in_features]
CUDATensor bias;    // [out_features]
size_t in_features, out_features;
cublasHandle_t cublas_handle;

public:
CUDALinear(size_t in_feat, size_t out_feat, cublasHandle_t handle)
: in_features(in_feat), out_features(out_feat), cublas_handle(handle) {

```
    weight = CUDATensor({out_feat, in_feat});
    bias = CUDATensor({out_feat});
    
    // Xavier initialization
    std::vector<float> w_init(out_feat * in_feat);
    std::mt19937 rng(std::random_device{}());
    float scale = std::sqrt(2.0f / (in_feat + out_feat));
    std::normal_distribution<float> dist(0.0f, scale);
    for (auto& v : w_init) v = dist(rng);
    weight.copy_from_host(w_init);
    bias.fill(0.0f);
}

CUDATensor forward(const CUDATensor& input, size_t batch_size) {
    CUDATensor output({batch_size, out_features});
    
    // Matrix multiplication: output = input @ weight^T
    // input: [batch_size, in_features]
    // weight: [out_features, in_features] -> weight^T: [in_features, out_features]
    // output: [batch_size, out_features]
    
    float alpha = 1.0f, beta = 0.0f;
    CUBLAS_CHECK(cublasSgemm(
        cublas_handle,
        CUBLAS_OP_T, CUBLAS_OP_N,
        out_features, batch_size, in_features,
        &alpha,
        weight.data(), in_features,
        input.data(), in_features,
        &beta,
        output.data(), out_features
    ));
    
    // Add bias (broadcast)
    // TODO: Implement efficient bias addition kernel
    
    return output;
}
```

};

// ============================================================================
// CUDA Layer Normalization
// ============================================================================

class CUDALayerNorm {
private:
CUDATensor gamma, beta;
size_t features;
float eps;

public:
CUDALayerNorm(size_t feat) : features(feat), eps(1e-5f) {
gamma = CUDATensor({feat});
beta = CUDATensor({feat});
gamma.fill(1.0f);
beta.fill(0.0f);
}

```
CUDATensor forward(const CUDATensor& input, size_t batch_size) {
    CUDATensor output({batch_size, features});
    
    dim3 block(features);
    dim3 grid(batch_size);
    
    layer_norm_kernel<<<grid, block>>>(
        output.data(), input.data(),
        gamma.data(), beta.data(),
        batch_size, features, eps
    );
    CUDA_CHECK(cudaGetLastError());
    
    return output;
}
```

};

// ============================================================================
// CUDA Global Workspace
// ============================================================================

class CUDAGlobalWorkspace {
private:
std::unique_ptr<CUDALinear> enc1, enc2, enc3;
std::unique_ptr<CUDALinear> dec1, dec2, dec3;
std::unique_ptr<CUDALayerNorm> norm1, norm2, norm3, norm4;
size_t input_dim, hidden_dim;

public:
CUDAGlobalWorkspace(size_t in_dim, size_t h_dim, cublasHandle_t handle)
: input_dim(in_dim), hidden_dim(h_dim) {

```
    enc1 = std::make_unique<CUDALinear>(in_dim, 256, handle);
    enc2 = std::make_unique<CUDALinear>(256, 128, handle);
    enc3 = std::make_unique<CUDALinear>(128, h_dim, handle);
    
    dec1 = std::make_unique<CUDALinear>(h_dim, 128, handle);
    dec2 = std::make_unique<CUDALinear>(128, 256, handle);
    dec3 = std::make_unique<CUDALinear>(256, in_dim, handle);
    
    norm1 = std::make_unique<CUDALayerNorm>(256);
    norm2 = std::make_unique<CUDALayerNorm>(128);
    norm3 = std::make_unique<CUDALayerNorm>(128);
    norm4 = std::make_unique<CUDALayerNorm>(256);
}

struct Output {
    CUDATensor h_global;
    CUDATensor x_recon;
};

Output forward(const CUDATensor& x, size_t batch_size) {
    // Encoder
    auto h1 = enc1->forward(x, batch_size);
    h1 = norm1->forward(h1, batch_size);
    h1.apply_gelu();
    
    auto h2 = enc2->forward(h1, batch_size);
    h2 = norm2->forward(h2, batch_size);
    h2.apply_gelu();
    
    auto h_global = enc3->forward(h2, batch_size);
    
    // Decoder
    auto d1 = dec1->forward(h_global, batch_size);
    d1 = norm3->forward(d1, batch_size);
    d1.apply_gelu();
    
    auto d2 = dec2->forward(d1, batch_size);
    d2 = norm4->forward(d2, batch_size);
    d2.apply_gelu();
    
    auto x_recon = dec3->forward(d2, batch_size);
    
    return {std::move(h_global), std::move(x_recon)};
}
```

};

// ============================================================================
// Main CUDA Validation System
// ============================================================================

class CUDAFDQCValidationSystem {
private:
cublasHandle_t cublas_handle;
std::unique_ptr<CUDAGlobalWorkspace> global_workspace;
size_t input_dim, global_dim, wm_dim, n_classes;

public:
CUDAFDQCValidationSystem(size_t in_dim, size_t g_dim, size_t w_dim, size_t n_cls)
: input_dim(in_dim), global_dim(g_dim), wm_dim(w_dim), n_classes(n_cls) {

```
    CUBLAS_CHECK(cublasCreate(&cublas_handle));
    global_workspace = std::make_unique<CUDAGlobalWorkspace>(in_dim, g_dim, cublas_handle);
}

~CUDAFDQCValidationSystem() {
    cublasDestroy(cublas_handle);
}

float compute_energy() const {
    return static_cast<float>(wm_dim * wm_dim);
}
```

};

// ============================================================================
// Main Function
// ============================================================================

int main(int argc, char** argv) {
std::cout << “=== FDQC v3.1 CUDA-Accelerated Validation ===” << std::endl;

```
// Check CUDA device
int device_count;
CUDA_CHECK(cudaGetDeviceCount(&device_count));
if (device_count == 0) {
    std::cerr << "No CUDA devices found!" << std::endl;
    return 1;
}

cudaDeviceProp prop;
CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));
std::cout << "Using GPU: " << prop.name << std::endl;
std::cout << "Compute Capability: " << prop.major << "." << prop.minor << std::endl;
std::cout << "Memory: " << prop.totalGlobalMem / (1024*1024*1024) << " GB\n" << std::endl;

// Configuration
const size_t INPUT_DIM = 784;
const size_t GLOBAL_DIM = 60;
const size_t N_CLASSES = 10;
const size_t BATCH_SIZE = 256;  // Larger batch for GPU

std::vector<size_t> capacities = {4, 6, 9, 12};

// Run experiments
for (size_t n : capacities) {
    std::cout << "\nTraining with n_WM = " << n << std::endl;
    std::cout << "Energy cost: " << (n * n) << std::endl;
    
    auto start = std::chrono::high_resolution_clock::now();
    
    CUDAFDQCValidationSystem model(INPUT_DIM, GLOBAL_DIM, n, N_CLASSES);
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    std::cout << "Model initialization: " << duration.count() << " ms" << std::endl;
}

std::cout << "\nCUDA validation complete." << std::endl;
return 0;
```

}